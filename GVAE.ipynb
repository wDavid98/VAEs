{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerías\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import cv2 as cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import glob\n",
    "\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uso de GPU\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET DE MICROGRAFÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes secas: 931  - Imágenes Húmedas:  792\n",
      "MF1: - secas:  379 - húmedas:  265\n",
      "MF2: - secas:  289 - húmedas:  265\n",
      "MF3: - secas:  263 - húmedas:  262\n"
     ]
    }
   ],
   "source": [
    "### Rutas de todas las imágenes\n",
    "\n",
    "## Obtener todas las rutas de las imágenes en el dataset\n",
    "ruta_dataset  = glob.glob('/home/willy98/archivos/Datasets/SH_procesadas/Recortadas_grises/*/*/*/*.jpg')\n",
    "\n",
    "## Capturar solo rutas secas \n",
    "rutas_imagenes_secas = []\n",
    "rutas_imagenes_humedas = []\n",
    "\n",
    "clases_secas= []\n",
    "clases_humedas = []\n",
    "for ruta in ruta_dataset:\n",
    "    if ruta.split('/')[-2] == 'Secas' or ruta.split('/')[-2] == 'Seca':\n",
    "        clases_secas.append(ruta.split('/')[-4])\n",
    "        rutas_imagenes_secas.append(ruta)\n",
    "    else:\n",
    "        clases_humedas.append(ruta.split('/')[-4])\n",
    "        rutas_imagenes_humedas.append(ruta)\n",
    "\n",
    "## Construcción de datasets\n",
    "data_secas = pd.DataFrame({'ruta':rutas_imagenes_secas,'clase':clases_secas})\n",
    "data_humedas = pd.DataFrame({'ruta':rutas_imagenes_humedas,'clase':clases_humedas})\n",
    "\n",
    "## reemplazo de etiquetas}\n",
    "lables_generalization = {'CuNi1':'MF1','CuNi2':'MF2','CuNi3':'MF3'}\n",
    "data_secas.replace(lables_generalization,inplace=True)\n",
    "data_humedas.replace(lables_generalization,inplace=True)\n",
    "\n",
    "## summary    \n",
    "print('Imágenes secas:', np.shape(rutas_imagenes_secas)[0],' - Imágenes Húmedas: ',np.shape(rutas_imagenes_humedas)[0])\n",
    "print('MF1:', '- secas: ', np.shape(data_secas[data_secas['clase']=='MF1'])[0], '- húmedas: ', np.shape(data_humedas[data_humedas['clase']=='MF1'])[0])\n",
    "print('MF2:', '- secas: ', np.shape(data_secas[data_secas['clase']=='MF2'])[0], '- húmedas: ', np.shape(data_humedas[data_humedas['clase']=='MF2'])[0])\n",
    "print('MF3:', '- secas: ', np.shape(data_secas[data_secas['clase']=='MF3'])[0], '- húmedas: ', np.shape(data_humedas[data_humedas['clase']=='MF3'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construir Datasets\n",
    "\n",
    "# Imágenes secas\n",
    "secas_arrays = []\n",
    "secas_labels = []\n",
    "\n",
    "## Dimensión de las imágenes\n",
    "nx = 128\n",
    "ny = 128\n",
    "\n",
    "## recorrer rutas\n",
    "for i in range(len(data_secas)):    \n",
    "    ## Capturar imágenes \n",
    "    ruta = data_secas.iloc[i].ruta\n",
    "    label = data_secas.iloc[i].clase\n",
    "    \n",
    "    ## Leer imágenes en 128x128\n",
    "    img = tf.keras.preprocessing.image.load_img(ruta, target_size=(nx, ny),color_mode = \"grayscale\")\n",
    "    \n",
    "    ## Convertir a array\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    ## Normalizar\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    ## Agregar a listas\n",
    "    secas_arrays.append(img_array)\n",
    "    secas_labels.append(label)\n",
    "    \n",
    "#imagenes húmedas\n",
    "humedas_arrays = []\n",
    "humedas_labels = []\n",
    "\n",
    "## recorrer rutas\n",
    "for i in range(len(data_humedas)):\n",
    "     ## Capturar imágenes \n",
    "    ruta = data_secas.iloc[i].ruta\n",
    "    label = data_secas.iloc[i].clase\n",
    "    \n",
    "    ## Leer imágenes en 128x128\n",
    "    img = tf.keras.preprocessing.image.load_img(ruta, target_size=(nx, ny),color_mode = \"grayscale\")\n",
    "    \n",
    "    ## Convertir a array\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    ## Normalizar\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    ## Agregar a listas\n",
    "    humedas_arrays.append(img_array)\n",
    "    humedas_labels.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secas: *training:  (744, 128, 128, 1) test:  (187, 128, 128, 1)\n",
      "Humedas: *training:  (633, 128, 128, 1) test:  (159, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "### Construcción del dataset de entrenamiento y de prueba\n",
    "X_train_secas, X_test_secas, y_train_secas, y_test_secas = train_test_split(secas_arrays, secas_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train_humedas, X_test_humedas, y_train_humedas, y_test_humedas = train_test_split(humedas_arrays, humedas_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Secas: *training: \",np.shape(X_train_secas), \"test: \", np.shape(X_test_secas))\n",
    "print(\"Humedas: *training: \",np.shape(X_train_humedas), \"test: \", np.shape(X_test_humedas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (744, 128, 128, 1)\n",
      "Number of images in x_train 744 y_train: (744,)\n",
      "Number of images in x_test (187, 128, 128, 1) y_test:  (187,)\n"
     ]
    }
   ],
   "source": [
    "x_train_secas = np.array(X_train_secas)\n",
    "x_test_secas = np.array(X_test_secas)\n",
    "\n",
    "x_train_secas = x_train_secas.astype('float32')\n",
    "x_test_secas= x_test_secas.astype('float32')\n",
    " \n",
    "#y_test = tf.keras.utils.to_categorical(y_test_secas)\n",
    "#y_train_secas = tf.keras.utils.to_categorical(y_train_secas) \n",
    "\n",
    "#x_train_secas /= 255\n",
    "#x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train_secas.shape)\n",
    "print('Number of images in x_train', x_train_secas.shape[0], \"y_train:\", np.shape(y_train_secas))\n",
    "print('Number of images in x_test', x_test_secas.shape, \"y_test: \", np.shape(y_test_secas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>  :  (744, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train_secas),' : ',np.shape(x_train_secas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOENCODER CONVENCIONAL (AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "nfilts = 512\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(128, 128,1))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts, (3, 3), activation='relu', padding='same',name='layer_E1')(encoder_inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/2, (3, 3), activation='relu', padding='same',name='layer_E3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/4, (3, 3), activation='relu', padding='same',name='layer_E2')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/8, (3, 3), activation='relu', padding='same',name='layer_E7')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(latent_dim, activation=\"relu\",name='encoded')(x)\n",
    "\n",
    "# Reshape para reconstruir la última convolucional\n",
    "ndim = 16\n",
    "x = layers.Reshape((ndim,ndim*2,1))(outputs)\n",
    "\n",
    "## Capas convolucionales\n",
    "x = tf.keras.layers.Conv2D(nfilts, (3, 3), activation='relu', padding='same',name='layer_D1')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/2, (3, 3), activation='relu', padding='same',name='layer_D2')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/4, (3, 3), activation='relu', padding='same',name='layer_D4')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 1))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "decoder_outputs = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same',name='decoded')(x)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(encoder_inputs, decoder_outputs)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_secas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento (Secas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate = 0.000001)\n",
    "autoencoder.compile(optimizer=opt,loss=\"binary_crossentropy\")\n",
    "autoencoder.fit(X_train_secas, X_train_secas, epochs=2000,\n",
    "                batch_size=32, shuffle=True,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=120)],\n",
    "                         validation_data=(X_test_secas, X_test_secas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('models/AE_dry_1_128.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento (Húmedas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate = 0.000001)\n",
    "autoencoder.compile(optimizer=opt,loss=\"binary_crossentropy\")\n",
    "autoencoder.fit(X_train_secas, X_train_secas, epochs=2000,\n",
    "                batch_size=32, shuffle=True,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=120)],\n",
    "                         validation_data=(X_test_secas, X_test_secas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('models/AE_wet_1_128.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOENCODER VARIACIONAL GAUSSIANO (GVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")        \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            \n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction /= tf.reduce_max(reconstruction)\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            '''\n",
    "            reconstruction_loss =tf.reduce_mean(\n",
    "                tf.reduce_sum( tf.keras.losses.MeanSquaredError()(data, reconstruction)\n",
    "                             )\n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.reconstruction = reconstruction\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)       \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),            \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 1  0           []                               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 01:11:52.630651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8584 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:1b:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " layer_E1 (Conv2D)              (None, 128, 128, 51  5120        ['input_1[0][0]']                \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 64, 512)  0           ['layer_E1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 512)  2048       ['max_pooling2d[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " layer_E3 (Conv2D)              (None, 64, 64, 256)  1179904     ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0          ['layer_E3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 256)  1024       ['max_pooling2d_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_E2 (Conv2D)              (None, 32, 32, 128)  295040      ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0          ['layer_E2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 16, 128)  512        ['max_pooling2d_2[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_E7 (Conv2D)              (None, 16, 16, 64)   73792       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 16384)        0           ['layer_E7[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 128)          2097280     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 128)          2097280     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 128)          0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,752,000\n",
      "Trainable params: 5,750,208\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "nfilts = 512\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(nx, ny,1))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts, (3, 3), activation='relu', padding='same',name='layer_E1')(encoder_inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/2, (3, 3), activation='relu', padding='same',name='layer_E3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/4, (3, 3), activation='relu', padding='same',name='layer_E2')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/8, (3, 3), activation='relu', padding='same',name='layer_E7')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16384)             2113536   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " layer_D1 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 16, 16, 256)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " layer_D2 (Conv2D)           (None, 16, 16, 128)       295040    \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 32, 32, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " layer_D4 (Conv2D)           (None, 32, 32, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 128, 128, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " layer_D6 (Conv2D)           (None, 128, 128, 32)      18464     \n",
      "                                                                 \n",
      " layer_D7 (Conv2D)           (None, 128, 128, 1)       289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,092,993\n",
      "Trainable params: 3,092,097\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Entrada Z\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "##dimensiones\n",
    "ndim = 8\n",
    "nfilts = 256\n",
    "\n",
    "## anti-flatten de la última capa convolucional\n",
    "x = layers.Dense(ndim * ndim * nfilts, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "## Reshape para reconstruir la última convolucional\n",
    "x = layers.Reshape((ndim,ndim, nfilts))(x)\n",
    "\n",
    "## Capas convolucionales\n",
    "x = tf.keras.layers.Conv2D(nfilts, (3, 3), activation='relu', padding='same',name='layer_D1')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/2, (3, 3), activation='relu', padding='same',name='layer_D2')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/4, (3, 3), activation='relu', padding='same',name='layer_D4')(x)\n",
    "x = tf.keras.layers.UpSampling2D((4, 4))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/8, (3, 3), activation='relu', padding='same',name='layer_D6')(x)\n",
    "\n",
    "decoder_outputs = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same',name='layer_D7')(x)\n",
    "\n",
    "## Construcción del decoder\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento (Secas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 01:11:56.650054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-31 01:11:58.622939: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f84b7ffef20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-31 01:11:58.622987: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2023-07-31 01:11:58.629654: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-31 01:11:58.805835: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 13s 118ms/step - loss: 34534.2962 - reconstruction_loss: 34842.3789 - kl_loss: 0.0698\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 34456.6114 - reconstruction_loss: 34683.9258 - kl_loss: 0.0691\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 34698.7811 - reconstruction_loss: 34501.2070 - kl_loss: 0.0685\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 34035.6602 - reconstruction_loss: 34120.8008 - kl_loss: 0.0685\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 33891.2461 - reconstruction_loss: 33958.3945 - kl_loss: 0.0680\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 34040.1905 - reconstruction_loss: 33633.1562 - kl_loss: 0.0677\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 33023.4638 - reconstruction_loss: 33275.0820 - kl_loss: 0.0701\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 33167.0217 - reconstruction_loss: 33081.1406 - kl_loss: 0.0713\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 33074.2247 - reconstruction_loss: 32832.4766 - kl_loss: 0.0734\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 32365.9649 - reconstruction_loss: 32513.2051 - kl_loss: 0.0768\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 32423.2474 - reconstruction_loss: 32396.7832 - kl_loss: 0.0807\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 32588.9714 - reconstruction_loss: 32415.6172 - kl_loss: 0.0832\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 32269.4825 - reconstruction_loss: 32226.8594 - kl_loss: 0.0854\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 31927.0506 - reconstruction_loss: 31861.0000 - kl_loss: 0.0910\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 31410.7708 - reconstruction_loss: 31695.4824 - kl_loss: 0.0963\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 31485.2005 - reconstruction_loss: 31338.8848 - kl_loss: 0.1041\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 31242.0902 - reconstruction_loss: 30827.7109 - kl_loss: 0.1098\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 30543.7872 - reconstruction_loss: 30841.3340 - kl_loss: 0.1144\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 30575.2125 - reconstruction_loss: 30720.3770 - kl_loss: 0.1207\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 30797.5511 - reconstruction_loss: 30652.3672 - kl_loss: 0.1300\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 30738.8803 - reconstruction_loss: 30584.7559 - kl_loss: 0.1382\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 30355.4945 - reconstruction_loss: 30473.2734 - kl_loss: 0.1473\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 30259.1516 - reconstruction_loss: 30338.4434 - kl_loss: 0.1579\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 29932.4947 - reconstruction_loss: 29904.4629 - kl_loss: 0.1705\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 29791.4749 - reconstruction_loss: 29922.3672 - kl_loss: 0.1841\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 29329.4220 - reconstruction_loss: 29392.4980 - kl_loss: 0.2050\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 29748.0690 - reconstruction_loss: 29592.4160 - kl_loss: 0.2285\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 29355.8581 - reconstruction_loss: 29335.2676 - kl_loss: 0.2534\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 29898.1444 - reconstruction_loss: 29637.2832 - kl_loss: 0.2741\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 28856.1443 - reconstruction_loss: 29033.2734 - kl_loss: 0.2988\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 28868.9530 - reconstruction_loss: 29039.2812 - kl_loss: 0.3263\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 28778.2666 - reconstruction_loss: 28704.7012 - kl_loss: 0.3616\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 28330.8576 - reconstruction_loss: 28481.0391 - kl_loss: 0.4049\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 28749.1063 - reconstruction_loss: 28562.4922 - kl_loss: 0.4491\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 28396.0466 - reconstruction_loss: 28358.2031 - kl_loss: 0.5024\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 28068.7865 - reconstruction_loss: 28087.9062 - kl_loss: 0.5736\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27912.4033 - reconstruction_loss: 27863.8672 - kl_loss: 0.6544\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27992.3815 - reconstruction_loss: 27954.6777 - kl_loss: 0.7369\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27907.7793 - reconstruction_loss: 27817.5566 - kl_loss: 0.8396\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27251.2177 - reconstruction_loss: 27289.9551 - kl_loss: 0.9654\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27189.8280 - reconstruction_loss: 27249.9844 - kl_loss: 1.0999\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 27105.7556 - reconstruction_loss: 27206.4434 - kl_loss: 1.2381\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 26891.6569 - reconstruction_loss: 27079.9707 - kl_loss: 1.4122\n",
      "Epoch 44/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 26969.8852 - reconstruction_loss: 26876.6934 - kl_loss: 1.6564\n",
      "Epoch 45/2000\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 26502.4648 - reconstruction_loss: 26432.9004 - kl_loss: 1.9410\n",
      "Epoch 46/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 26102.1598 - reconstruction_loss: 26180.2363 - kl_loss: 2.2979\n",
      "Epoch 47/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 26107.5247 - reconstruction_loss: 25965.1250 - kl_loss: 2.6829\n",
      "Epoch 48/2000\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 25895.0422 - reconstruction_loss: 25862.8457 - kl_loss: 3.2120\n",
      "Epoch 49/2000\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 25778.0473 - reconstruction_loss: 25655.0625 - kl_loss: 3.8631\n",
      "Epoch 50/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 25107.2776 - reconstruction_loss: 25223.9473 - kl_loss: 4.6163\n",
      "Epoch 51/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 25109.5693 - reconstruction_loss: 25022.9004 - kl_loss: 5.5109\n",
      "Epoch 52/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 24802.6245 - reconstruction_loss: 24713.1777 - kl_loss: 6.6308\n",
      "Epoch 53/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 24409.7845 - reconstruction_loss: 24357.1250 - kl_loss: 8.2620\n",
      "Epoch 54/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 24365.7651 - reconstruction_loss: 24096.8750 - kl_loss: 10.1950\n",
      "Epoch 55/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 23575.9682 - reconstruction_loss: 23455.5957 - kl_loss: 12.9604\n",
      "Epoch 56/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 23059.1078 - reconstruction_loss: 22842.4160 - kl_loss: 16.5122\n",
      "Epoch 57/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 22641.5313 - reconstruction_loss: 22497.2109 - kl_loss: 21.1996\n",
      "Epoch 58/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 21402.6265 - reconstruction_loss: 21484.8613 - kl_loss: 27.1418\n",
      "Epoch 59/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 20815.7946 - reconstruction_loss: 20586.4062 - kl_loss: 35.5283\n",
      "Epoch 60/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 84ms/step - loss: 19662.8168 - reconstruction_loss: 19399.8535 - kl_loss: 46.4528\n",
      "Epoch 61/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 19022.1030 - reconstruction_loss: 18773.6621 - kl_loss: 59.4265\n",
      "Epoch 62/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 17920.1518 - reconstruction_loss: 17701.4590 - kl_loss: 75.7878\n",
      "Epoch 63/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 17199.3921 - reconstruction_loss: 16846.0508 - kl_loss: 95.1347\n",
      "Epoch 64/2000\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 16509.3927 - reconstruction_loss: 16240.3564 - kl_loss: 118.1768\n",
      "Epoch 65/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 15582.3377 - reconstruction_loss: 15416.1553 - kl_loss: 145.1729\n",
      "Epoch 66/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 15044.9579 - reconstruction_loss: 14695.6553 - kl_loss: 175.9499\n",
      "Epoch 67/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 14377.9254 - reconstruction_loss: 14148.7617 - kl_loss: 210.7634\n",
      "Epoch 68/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 14234.2082 - reconstruction_loss: 13775.1572 - kl_loss: 250.0004\n",
      "Epoch 69/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 13587.5812 - reconstruction_loss: 13295.0156 - kl_loss: 292.4292\n",
      "Epoch 70/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 13273.6246 - reconstruction_loss: 12855.2275 - kl_loss: 342.1698\n",
      "Epoch 71/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 13098.0581 - reconstruction_loss: 12602.4414 - kl_loss: 391.9920\n",
      "Epoch 72/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 12749.7800 - reconstruction_loss: 12214.5010 - kl_loss: 445.5301\n",
      "Epoch 73/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12540.0949 - reconstruction_loss: 11986.3057 - kl_loss: 499.4490\n",
      "Epoch 74/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12358.5694 - reconstruction_loss: 11785.4971 - kl_loss: 543.1669\n",
      "Epoch 75/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12336.1440 - reconstruction_loss: 11742.1289 - kl_loss: 584.7272\n",
      "Epoch 76/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12206.4486 - reconstruction_loss: 11531.4883 - kl_loss: 620.7004\n",
      "Epoch 77/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12186.0187 - reconstruction_loss: 11465.1836 - kl_loss: 656.9406\n",
      "Epoch 78/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12060.1021 - reconstruction_loss: 11402.9365 - kl_loss: 676.1622\n",
      "Epoch 79/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 12004.2846 - reconstruction_loss: 11324.8389 - kl_loss: 698.2210\n",
      "Epoch 80/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 12004.9371 - reconstruction_loss: 11256.1367 - kl_loss: 709.7813\n",
      "Epoch 81/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11890.7362 - reconstruction_loss: 11204.2959 - kl_loss: 708.3444\n",
      "Epoch 82/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11904.5412 - reconstruction_loss: 11212.3516 - kl_loss: 705.5974\n",
      "Epoch 83/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11825.1381 - reconstruction_loss: 11131.3818 - kl_loss: 699.4783\n",
      "Epoch 84/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11789.3670 - reconstruction_loss: 11123.6562 - kl_loss: 686.8621\n",
      "Epoch 85/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11786.8372 - reconstruction_loss: 11125.3281 - kl_loss: 682.5342\n",
      "Epoch 86/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11799.4584 - reconstruction_loss: 11118.7969 - kl_loss: 674.5013\n",
      "Epoch 87/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11736.6411 - reconstruction_loss: 11055.7930 - kl_loss: 667.4087\n",
      "Epoch 88/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11677.1120 - reconstruction_loss: 11023.5303 - kl_loss: 660.4041\n",
      "Epoch 89/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11684.3532 - reconstruction_loss: 11010.1240 - kl_loss: 651.1581\n",
      "Epoch 90/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11610.3500 - reconstruction_loss: 11002.2080 - kl_loss: 634.7219\n",
      "Epoch 91/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11616.8471 - reconstruction_loss: 10984.4688 - kl_loss: 621.1232\n",
      "Epoch 92/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11610.0794 - reconstruction_loss: 11018.5771 - kl_loss: 609.1467\n",
      "Epoch 93/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11522.2640 - reconstruction_loss: 10958.0732 - kl_loss: 603.3536\n",
      "Epoch 94/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11575.8341 - reconstruction_loss: 10990.9531 - kl_loss: 588.2129\n",
      "Epoch 95/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11519.1077 - reconstruction_loss: 10994.8164 - kl_loss: 577.9927\n",
      "Epoch 96/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11537.0318 - reconstruction_loss: 10940.7344 - kl_loss: 572.1945\n",
      "Epoch 97/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11490.1928 - reconstruction_loss: 10948.5518 - kl_loss: 558.8860\n",
      "Epoch 98/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11454.7004 - reconstruction_loss: 10944.6436 - kl_loss: 547.2251\n",
      "Epoch 99/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11450.3545 - reconstruction_loss: 10910.8818 - kl_loss: 544.3803\n",
      "Epoch 100/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11465.8522 - reconstruction_loss: 10926.6758 - kl_loss: 534.2482\n",
      "Epoch 101/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11445.8154 - reconstruction_loss: 10910.0752 - kl_loss: 525.2720\n",
      "Epoch 102/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11438.0359 - reconstruction_loss: 10910.3936 - kl_loss: 516.3152\n",
      "Epoch 103/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11427.7998 - reconstruction_loss: 10892.2178 - kl_loss: 508.3098\n",
      "Epoch 104/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11366.3656 - reconstruction_loss: 10876.4873 - kl_loss: 503.6007\n",
      "Epoch 105/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11361.6142 - reconstruction_loss: 10901.0088 - kl_loss: 495.8126\n",
      "Epoch 106/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11361.6091 - reconstruction_loss: 10861.7822 - kl_loss: 489.1628\n",
      "Epoch 107/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11376.1712 - reconstruction_loss: 10869.3721 - kl_loss: 482.2992\n",
      "Epoch 108/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11354.3102 - reconstruction_loss: 10874.0205 - kl_loss: 478.9009\n",
      "Epoch 109/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11353.9333 - reconstruction_loss: 10888.3281 - kl_loss: 472.0639\n",
      "Epoch 110/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11286.9659 - reconstruction_loss: 10843.4697 - kl_loss: 463.5767\n",
      "Epoch 111/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11349.9173 - reconstruction_loss: 10884.3271 - kl_loss: 452.7794\n",
      "Epoch 112/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11315.7657 - reconstruction_loss: 10865.7969 - kl_loss: 449.2696\n",
      "Epoch 113/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11263.0532 - reconstruction_loss: 10841.9170 - kl_loss: 444.4602\n",
      "Epoch 114/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11261.0117 - reconstruction_loss: 10825.8613 - kl_loss: 439.4875\n",
      "Epoch 115/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11250.5043 - reconstruction_loss: 10826.1943 - kl_loss: 431.1019\n",
      "Epoch 116/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11229.1259 - reconstruction_loss: 10816.2656 - kl_loss: 424.3144\n",
      "Epoch 117/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11225.2535 - reconstruction_loss: 10830.2061 - kl_loss: 414.6771\n",
      "Epoch 118/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 83ms/step - loss: 11243.9282 - reconstruction_loss: 10814.0557 - kl_loss: 407.5281\n",
      "Epoch 119/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11196.3018 - reconstruction_loss: 10801.7627 - kl_loss: 404.4466\n",
      "Epoch 120/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11226.3661 - reconstruction_loss: 10838.4365 - kl_loss: 398.1804\n",
      "Epoch 121/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11166.6479 - reconstruction_loss: 10809.5205 - kl_loss: 395.5148\n",
      "Epoch 122/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11235.6950 - reconstruction_loss: 10816.3535 - kl_loss: 393.2694\n",
      "Epoch 123/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11154.7629 - reconstruction_loss: 10794.8877 - kl_loss: 386.8766\n",
      "Epoch 124/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11204.2972 - reconstruction_loss: 10808.9346 - kl_loss: 381.2271\n",
      "Epoch 125/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11151.2036 - reconstruction_loss: 10788.1182 - kl_loss: 373.6667\n",
      "Epoch 126/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11132.0736 - reconstruction_loss: 10784.5156 - kl_loss: 370.3252\n",
      "Epoch 127/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11166.9039 - reconstruction_loss: 10796.7354 - kl_loss: 362.6388\n",
      "Epoch 128/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11152.6125 - reconstruction_loss: 10773.7471 - kl_loss: 357.3507\n",
      "Epoch 129/2000\n",
      "24/24 [==============================] - 2s 85ms/step - loss: 11166.9654 - reconstruction_loss: 10793.6885 - kl_loss: 354.0581\n",
      "Epoch 130/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11138.7319 - reconstruction_loss: 10806.9873 - kl_loss: 354.1505\n",
      "Epoch 131/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11127.2660 - reconstruction_loss: 10785.7852 - kl_loss: 349.3792\n",
      "Epoch 132/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11159.2292 - reconstruction_loss: 10784.7744 - kl_loss: 345.6992\n",
      "Epoch 133/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11116.6479 - reconstruction_loss: 10774.0752 - kl_loss: 340.0269\n",
      "Epoch 134/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11096.8232 - reconstruction_loss: 10777.5693 - kl_loss: 336.0818\n",
      "Epoch 135/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11070.1258 - reconstruction_loss: 10768.0283 - kl_loss: 332.7794\n",
      "Epoch 136/2000\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 11077.2248 - reconstruction_loss: 10761.9238 - kl_loss: 326.0500\n",
      "Epoch 137/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11068.8092 - reconstruction_loss: 10766.5986 - kl_loss: 320.5156\n",
      "Epoch 138/2000\n",
      "24/24 [==============================] - 2s 84ms/step - loss: 11082.3810 - reconstruction_loss: 10776.2236 - kl_loss: 316.3592\n",
      "Epoch 139/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11103.7926 - reconstruction_loss: 10768.1123 - kl_loss: 314.5105\n",
      "Epoch 140/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11083.1871 - reconstruction_loss: 10785.5977 - kl_loss: 315.4626\n",
      "Epoch 141/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11056.9598 - reconstruction_loss: 10747.7568 - kl_loss: 310.0915\n",
      "Epoch 142/2000\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 11075.0879 - reconstruction_loss: 10753.9102 - kl_loss: 305.9880\n",
      "Epoch 143/2000\n",
      "12/24 [==============>...............] - ETA: 1s - loss: 11026.1499 - reconstruction_loss: 10733.4385 - kl_loss: 302.4334"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.000001)\n",
    "#nll = lambda x , rv_x: -rv_x.log_prob(x)\n",
    "vae.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "vae.fit(x_train_secas, epochs=2000, \n",
    "                batch_size=32,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guardar modelo\n",
    "vae.encoder.save('models/GVAE_encoder_dry_1_128.h5')\n",
    "vae.decoder.save('models/GVAE_decoder_dry_1_128.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento (Húmedas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.000001)\n",
    "#nll = lambda x , rv_x: -rv_x.log_prob(x)\n",
    "vae.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "vae.fit(X_train_secas, epochs=2000, \n",
    "                batch_size=32,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guardar modelo\n",
    "vae.encoder.save('models/GVAE_encoder_wet_1_128.h5')\n",
    "vae.decoder.save('models/GVAE_decoder_wet_1_128.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
