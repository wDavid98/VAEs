{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5ccb6f",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55cc98f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 03:26:28.885346: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 03:26:28.934109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "## Librerías\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import cv2 as cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "64\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfdf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uso de GPU\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb1e24",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48ca6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes secas: 927\n",
      "MF1: - secas:  378\n",
      "MF2: - secas:  288\n",
      "MF3: - secas:  261\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>route_image</th>\n",
       "      <th>class</th>\n",
       "      <th>number</th>\n",
       "      <th>route_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>IMG_20230707_173506_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>IMG_20230707_191821_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>IMG_20230707_171027_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>IMG_20230707_184632_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>IMG_20230707_161859_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>IMG_20230711_212837_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>IMG_20230711_212453_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>IMG_20230711_211442_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>IMG_20230707_170821_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>IMG_20230711_180500_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>927 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name   \n",
       "258  IMG_20230707_173506_jpg  \\\n",
       "464  IMG_20230707_191821_jpg   \n",
       "235  IMG_20230707_171027_jpg   \n",
       "376  IMG_20230707_184632_jpg   \n",
       "108  IMG_20230707_161859_jpg   \n",
       "..                       ...   \n",
       "841  IMG_20230711_212837_jpg   \n",
       "827  IMG_20230711_212453_jpg   \n",
       "790  IMG_20230711_211442_jpg   \n",
       "218  IMG_20230707_170821_jpg   \n",
       "709  IMG_20230711_180500_jpg   \n",
       "\n",
       "                                           route_image class number   \n",
       "258  /home/willy98/archivos/Datasets/Micrografías_s...     0      4  \\\n",
       "464  /home/willy98/archivos/Datasets/Micrografías_s...     0      7   \n",
       "235  /home/willy98/archivos/Datasets/Micrografías_s...     0      3   \n",
       "376  /home/willy98/archivos/Datasets/Micrografías_s...     0      6   \n",
       "108  /home/willy98/archivos/Datasets/Micrografías_s...     0      1   \n",
       "..                                                 ...   ...    ...   \n",
       "841  /home/willy98/archivos/Datasets/Micrografías_s...     2      7   \n",
       "827  /home/willy98/archivos/Datasets/Micrografías_s...     2      7   \n",
       "790  /home/willy98/archivos/Datasets/Micrografías_s...     2      2   \n",
       "218  /home/willy98/archivos/Datasets/Micrografías_s...     0      3   \n",
       "709  /home/willy98/archivos/Datasets/Micrografías_s...     1      6   \n",
       "\n",
       "                                            route_mask  \n",
       "258  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "464  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "235  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "376  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "108  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "..                                                 ...  \n",
       "841  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "827  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "790  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "218  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "709  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "\n",
       "[927 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Obtener todas las rutas de las imágenes en el dataset\n",
    "rutas_dataset  = glob.glob('/home/willy98/archivos/Datasets/Micrografías_segmentadas/secas/*/*/*/*.jpg')\n",
    "\n",
    "## Separar rutas\n",
    "mask_list = []\n",
    "image_list = []\n",
    "for ruta in rutas_dataset:\n",
    "    ruta_list = ruta.split('/')\n",
    "    clase = ruta_list[7]\n",
    "    number = ruta_list[8]\n",
    "    tipo = ruta_list[9]\n",
    "    name = ruta_list[10].split('.')[0]\n",
    "    \n",
    "    if tipo == 'train':\n",
    "        image_list.append([name,ruta,clase,number])\n",
    "    elif tipo == 'mask_bin':\n",
    "        mask_list.append([name,ruta,clase,number])\n",
    "    else:\n",
    "        None\n",
    "\n",
    "mask_pd = pd.DataFrame(mask_list,columns=['name','route_mask','class','number']).sort_values(by='name')\n",
    "image_pd = pd.DataFrame(image_list,columns=['name','route_image','class','number']).sort_values(by='name')\n",
    "\n",
    "routes_dataset = image_pd.merge(mask_pd,on=['name','class','number'],how='left')\n",
    "\n",
    "routes_dataset.dropna(inplace=True)\n",
    "\n",
    "routes_dataset = routes_dataset.sample(frac = 1)\n",
    "\n",
    "print('Imágenes secas:', np.shape(routes_dataset)[0])\n",
    "print('MF1:', '- secas: ', np.shape(routes_dataset[routes_dataset['class']=='MF1'])[0])\n",
    "print('MF2:', '- secas: ', np.shape(routes_dataset[routes_dataset['class']=='MF2'])[0])\n",
    "print('MF3:', '- secas: ', np.shape(routes_dataset[routes_dataset['class']=='MF3'])[0])\n",
    "\n",
    "char_to_num = {'MF1':'0','MF2':'1','MF3':'2'}\n",
    "\n",
    "routes_dataset['class'].replace(char_to_num,inplace=True)\n",
    "\n",
    "routes_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939e1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase MF1: train:  303 , test:  75\n",
      "Clase MF2: train:  231 , test:  57\n",
      "Clase MF3: train:  209 , test:  52\n"
     ]
    }
   ],
   "source": [
    "## Establecer el porcentaje de separación\n",
    "prct_train = 0.8\n",
    "\n",
    "thold1 = int(np.ceil(prct_train * len(routes_dataset[routes_dataset['class']=='0'])))\n",
    "thold2 = int(np.ceil(prct_train * len(routes_dataset[routes_dataset['class']=='1'])))\n",
    "thold3 = int(np.ceil(prct_train * len(routes_dataset[routes_dataset['class']=='2'])))\n",
    "\n",
    "## Separar por clases para entrenamiento\n",
    "train_cu1 = routes_dataset[routes_dataset['class'] == '0'][0:thold1]\n",
    "train_cu2 = routes_dataset[routes_dataset['class'] == '1'][0:thold2]\n",
    "train_cu3 = routes_dataset[routes_dataset['class'] == '2'][0:thold3]\n",
    "\n",
    "## tomar las demás imágenes para test\n",
    "test_cu1 = routes_dataset[routes_dataset['class'] == '0'][thold1:]\n",
    "test_cu2 = routes_dataset[routes_dataset['class'] == '1'][thold2:]\n",
    "test_cu3 = routes_dataset[routes_dataset['class'] == '2'][thold3:]\n",
    "\n",
    "## Cantidad de muestras por clase\n",
    "print('Clase MF1: train: ',len(train_cu1),', test: ',len(test_cu1))\n",
    "print('Clase MF2: train: ',len(train_cu2),', test: ',len(test_cu2))\n",
    "print('Clase MF3: train: ',len(train_cu3),', test: ',len(test_cu3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da92bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>route_image</th>\n",
       "      <th>class</th>\n",
       "      <th>number</th>\n",
       "      <th>route_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>IMG_20230707_173506_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>IMG_20230707_191821_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>IMG_20230707_171027_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>IMG_20230707_184632_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>IMG_20230707_161859_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>IMG_20230707_164706_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>IMG_20230707_180420_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>IMG_20230707_191248_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>IMG_20230707_173702_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>IMG_20230707_185126_jpg</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/home/willy98/archivos/Datasets/Micrografías_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name   \n",
       "258  IMG_20230707_173506_jpg  \\\n",
       "464  IMG_20230707_191821_jpg   \n",
       "235  IMG_20230707_171027_jpg   \n",
       "376  IMG_20230707_184632_jpg   \n",
       "108  IMG_20230707_161859_jpg   \n",
       "..                       ...   \n",
       "202  IMG_20230707_164706_jpg   \n",
       "318  IMG_20230707_180420_jpg   \n",
       "425  IMG_20230707_191248_jpg   \n",
       "272  IMG_20230707_173702_jpg   \n",
       "408  IMG_20230707_185126_jpg   \n",
       "\n",
       "                                           route_image class number   \n",
       "258  /home/willy98/archivos/Datasets/Micrografías_s...     0      4  \\\n",
       "464  /home/willy98/archivos/Datasets/Micrografías_s...     0      7   \n",
       "235  /home/willy98/archivos/Datasets/Micrografías_s...     0      3   \n",
       "376  /home/willy98/archivos/Datasets/Micrografías_s...     0      6   \n",
       "108  /home/willy98/archivos/Datasets/Micrografías_s...     0      1   \n",
       "..                                                 ...   ...    ...   \n",
       "202  /home/willy98/archivos/Datasets/Micrografías_s...     0      2   \n",
       "318  /home/willy98/archivos/Datasets/Micrografías_s...     0      5   \n",
       "425  /home/willy98/archivos/Datasets/Micrografías_s...     0      7   \n",
       "272  /home/willy98/archivos/Datasets/Micrografías_s...     0      4   \n",
       "408  /home/willy98/archivos/Datasets/Micrografías_s...     0      6   \n",
       "\n",
       "                                            route_mask  \n",
       "258  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "464  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "235  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "376  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "108  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "..                                                 ...  \n",
       "202  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "318  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "425  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "272  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "408  /home/willy98/archivos/Datasets/Micrografías_s...  \n",
       "\n",
       "[303 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Definir datasets de training y test\n",
    "# Todas las clases\n",
    "#train_df =  pd.concat([pd.concat([train_cu1,train_cu2]),train_cu3])\n",
    "#test_df = pd.concat([pd.concat([test_cu1,test_cu2]),test_cu3])\n",
    "\n",
    "# solo una clase\n",
    "train_df = train_cu1\n",
    "test_df = test_cu1\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7237bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 243 validated image filenames belonging to 1 classes.\n",
      "Found 243 validated image filenames belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "## Crear objetos transformadores\n",
    "## Para entrenamiento\n",
    "SEED = 100\n",
    "\n",
    "img_generator_images = tf.keras.preprocessing.image.ImageDataGenerator(  \n",
    "    rescale=1./255,      \n",
    "    horizontal_flip=True,\n",
    "    rotation_range=90,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "img_generator_mask = tf.keras.preprocessing.image.ImageDataGenerator(  \n",
    "    rescale=1./255,      \n",
    "    horizontal_flip=True,\n",
    "    rotation_range=90,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "## Crear objeto generador de imágenes a partir del objeto transformator\n",
    "## Pra entrenamiento\n",
    "train_generator_IMAGES = img_generator_images.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    #directory=\"./train/\",\n",
    "    x_col=\"route_image\",\n",
    "    y_col=\"class\",\n",
    "    labels = ['MF1'],\n",
    "    #labels = ['MF2'],\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    "    class_mode=\"sparse\" ,\n",
    "    validation_split=0.2,\n",
    "    color_mode =\"grayscale\",\n",
    "    target_size=(128,128))\n",
    "\n",
    "train_generator_MASK = img_generator_mask.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    #directory=\"./train/\",\n",
    "    x_col=\"route_mask\",\n",
    "    y_col=\"class\",\n",
    "    labels = ['MF1'],\n",
    "    #labels = ['MF2'],\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    "    class_mode=\"sparse\",\n",
    "    validation_split=0.2,\n",
    "    color_mode =\"grayscale\",\n",
    "    target_size=(128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80d7d9",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee9760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self,encoder,decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        ##Variables que guardan paso a paso las pérdidas\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')        \n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return[\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def train_step(self,data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ## Generar embebidos\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            \n",
    "            ## Reconstruir embebidos\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            ## Normalizar reconstrucciones\n",
    "            reconstruction /= tf.reduce_max(reconstruction)\n",
    "            \n",
    "            ## Calcular la diferencia binaria entre las imágenes de entrada y las reconstrucciones\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )         \n",
    "            \n",
    "            '''\n",
    "            reconstruction_loss =tf.reduce_mean(\n",
    "                tf.reduce_sum( tf.keras.losses.MeanSquaredError()(data, reconstruction)\n",
    "                             )\n",
    "            )\n",
    "            '''\n",
    "            \n",
    "            ## Pérdida de kullback leibler\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))            \n",
    "        \n",
    "            ## Pérdida total\n",
    "            total_loss = reconstruction_loss + kl_loss     \n",
    "            \n",
    "        ## Calcular los gradientes \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)       \n",
    "\n",
    "        ## Guardar reconstrucciones en una variable propia del modelo\n",
    "        self.reconstruction = reconstruction\n",
    "\n",
    "        ## Calcular gradientes entre los pesos            \n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        ## Actualizar la variable de pérdida total\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "\n",
    "        ## Actualizar la variable de pérdida de reconstrucción\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)       \n",
    "\n",
    "        ## Actualizar la variable de pérdida por kull-back\n",
    "        self.kl_loss_tracker.update_state(kl_loss)       \n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(), \n",
    "            'kl_loss':self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d523407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " layer_E1 (Conv2D)              (None, 128, 128, 10  10240       ['input_5[0][0]']                \n",
      "                                24)                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 64, 64, 1024  0          ['layer_E1[0][0]']               \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 64, 1024  4096       ['max_pooling2d_10[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " layer_E3 (Conv2D)              (None, 64, 64, 1024  9438208     ['batch_normalization_16[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 32, 32, 1024  0          ['layer_E3[0][0]']               \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 32, 32, 1024  4096       ['max_pooling2d_11[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " layer_E2 (Conv2D)              (None, 32, 32, 512)  4719104     ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 16, 16, 512)  0          ['layer_E2[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 512)  2048       ['max_pooling2d_12[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " layer_E4 (Conv2D)              (None, 16, 16, 128)  589952      ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 8, 8, 128)   0           ['layer_E4[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 128)   512         ['max_pooling2d_13[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " layer_E6 (Conv2D)              (None, 8, 8, 64)     73792       ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (None, 4, 4, 64)    0           ['layer_E6[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 4, 4, 64)    256         ['max_pooling2d_14[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " layer_E7 (Conv2D)              (None, 4, 4, 16)     9232        ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 256)          0           ['layer_E7[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 128)          32896       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 128)          32896       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 128)          0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,917,328\n",
      "Trainable params: 14,911,824\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Encoder\n",
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(128, 128, 1))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same',name='layer_E1')(encoder_inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same',name='layer_E3')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same',name='layer_E2')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same',name='layer_E4')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',name='layer_E6')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same',name='layer_E7')(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aac1b1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8192)              1056768   \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " layer_D1 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 16, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_D2 (Conv2D)           (None, 16, 16, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 32, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_D4 (Conv2D)           (None, 32, 32, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 128, 128, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 128, 128, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_D6 (Conv2D)           (None, 128, 128, 16)      4624      \n",
      "                                                                 \n",
      " layer_D7 (Conv2D)           (None, 128, 128, 1)       145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,302,273\n",
      "Trainable params: 1,301,825\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Decoder\n",
    "## Entrada Z\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "##dimensiones\n",
    "ndim = 8\n",
    "nfilts = 128\n",
    "\n",
    "## anti-flatten de la última capa convolucional\n",
    "x = layers.Dense(ndim * ndim * nfilts, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "## Reshape para reconstruir la última convolucional\n",
    "x = layers.Reshape((ndim,ndim, nfilts))(x)\n",
    "\n",
    "## Capas convolucionales\n",
    "x = tf.keras.layers.Conv2D(nfilts, (3, 3), activation='relu', padding='same',name='layer_D1')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/2, (3, 3), activation='relu', padding='same',name='layer_D2')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/4, (3, 3), activation='relu', padding='same',name='layer_D4')(x)\n",
    "x = tf.keras.layers.UpSampling2D((4, 4))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(nfilts/8, (3, 3), activation='relu', padding='same',name='layer_D6')(x)\n",
    "\n",
    "decoder_outputs = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same',name='layer_D7')(x)\n",
    "\n",
    "## Construcción del decoder\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b53ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 03:33:51.574916: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-10-10 03:34:02.716302: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.80GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-10-10 03:34:02.716357: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.80GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-10-10 03:34:07.957203: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.52GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 26s 1s/step - loss: 34519.9041 - reconstruction_loss: 34405.7031 - kl_loss: 0.0235\n",
      "Epoch 2/4000\n",
      "8/8 [==============================] - 4s 493ms/step - loss: 33543.6944 - reconstruction_loss: 33572.7500 - kl_loss: 0.0361\n",
      "Epoch 3/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 33608.7543 - reconstruction_loss: 33616.1406 - kl_loss: 0.0398\n",
      "Epoch 4/4000\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 32655.0490 - reconstruction_loss: 32901.6992 - kl_loss: 0.0363\n",
      "Epoch 5/4000\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 32819.3804 - reconstruction_loss: 32308.9766 - kl_loss: 0.0496\n",
      "Epoch 6/4000\n",
      "8/8 [==============================] - 4s 494ms/step - loss: 31842.5911 - reconstruction_loss: 31578.1250 - kl_loss: 0.0746\n",
      "Epoch 7/4000\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 31214.1764 - reconstruction_loss: 30958.2285 - kl_loss: 0.1266\n",
      "Epoch 8/4000\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 31090.4920 - reconstruction_loss: 31026.5215 - kl_loss: 0.1683\n",
      "Epoch 9/4000\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 30279.1204 - reconstruction_loss: 30296.9668 - kl_loss: 0.2370\n",
      "Epoch 10/4000\n",
      "8/8 [==============================] - 4s 498ms/step - loss: 30895.2437 - reconstruction_loss: 30472.2656 - kl_loss: 0.3557\n",
      "Epoch 11/4000\n",
      "8/8 [==============================] - 4s 545ms/step - loss: 30095.2057 - reconstruction_loss: 29866.1562 - kl_loss: 0.4974\n",
      "Epoch 12/4000\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 29966.4731 - reconstruction_loss: 29820.3125 - kl_loss: 0.6283\n",
      "Epoch 13/4000\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 29762.4427 - reconstruction_loss: 29844.7324 - kl_loss: 0.8195\n",
      "Epoch 14/4000\n",
      "8/8 [==============================] - 4s 470ms/step - loss: 28915.5938 - reconstruction_loss: 28752.3496 - kl_loss: 1.2581\n",
      "Epoch 15/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 29441.1230 - reconstruction_loss: 28926.5898 - kl_loss: 2.4314\n",
      "Epoch 16/4000\n",
      "8/8 [==============================] - 4s 539ms/step - loss: 27506.2472 - reconstruction_loss: 27964.4336 - kl_loss: 4.8689\n",
      "Epoch 17/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 28273.6695 - reconstruction_loss: 28038.6211 - kl_loss: 9.2880\n",
      "Epoch 18/4000\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 27293.6359 - reconstruction_loss: 26992.2871 - kl_loss: 19.3866\n",
      "Epoch 19/4000\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 25364.2509 - reconstruction_loss: 24785.6660 - kl_loss: 49.8024\n",
      "Epoch 20/4000\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 22791.3537 - reconstruction_loss: 22064.6738 - kl_loss: 151.0268\n",
      "Epoch 21/4000\n",
      "8/8 [==============================] - 4s 480ms/step - loss: 18286.1797 - reconstruction_loss: 16964.7461 - kl_loss: 656.3151\n",
      "Epoch 22/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 16131.5706 - reconstruction_loss: 13297.4463 - kl_loss: 2946.9336\n",
      "Epoch 23/4000\n",
      "8/8 [==============================] - 4s 534ms/step - loss: 15524.9661 - reconstruction_loss: 13727.6191 - kl_loss: 1651.8523\n",
      "Epoch 24/4000\n",
      "8/8 [==============================] - 4s 488ms/step - loss: 15598.1159 - reconstruction_loss: 14154.6172 - kl_loss: 1178.7292\n",
      "Epoch 25/4000\n",
      "8/8 [==============================] - 4s 498ms/step - loss: 15353.4365 - reconstruction_loss: 13498.2012 - kl_loss: 1831.1158\n",
      "Epoch 26/4000\n",
      "8/8 [==============================] - 5s 573ms/step - loss: 14718.1224 - reconstruction_loss: 12779.9062 - kl_loss: 1841.0941\n",
      "Epoch 27/4000\n",
      "8/8 [==============================] - 5s 539ms/step - loss: 14249.0933 - reconstruction_loss: 13485.8828 - kl_loss: 1093.2726\n",
      "Epoch 28/4000\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 14189.6143 - reconstruction_loss: 12991.3750 - kl_loss: 1241.4856\n",
      "Epoch 29/4000\n",
      "8/8 [==============================] - 4s 496ms/step - loss: 13883.8601 - reconstruction_loss: 12335.2188 - kl_loss: 1436.0931\n",
      "Epoch 30/4000\n",
      "8/8 [==============================] - 4s 472ms/step - loss: 13519.7063 - reconstruction_loss: 12482.0762 - kl_loss: 1120.6233\n",
      "Epoch 31/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 13343.7705 - reconstruction_loss: 12393.5625 - kl_loss: 995.4089\n",
      "Epoch 32/4000\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 13639.9090 - reconstruction_loss: 12336.1992 - kl_loss: 1091.3873\n",
      "Epoch 33/4000\n",
      "8/8 [==============================] - 4s 492ms/step - loss: 13537.4418 - reconstruction_loss: 12281.0781 - kl_loss: 1200.1038\n",
      "Epoch 34/4000\n",
      "8/8 [==============================] - 4s 474ms/step - loss: 13111.5027 - reconstruction_loss: 11901.9131 - kl_loss: 1044.0947\n",
      "Epoch 35/4000\n",
      "8/8 [==============================] - 4s 475ms/step - loss: 13093.1565 - reconstruction_loss: 12226.8008 - kl_loss: 911.1927\n",
      "Epoch 36/4000\n",
      "8/8 [==============================] - 4s 497ms/step - loss: 12757.9257 - reconstruction_loss: 11906.3086 - kl_loss: 996.4977\n",
      "Epoch 37/4000\n",
      "8/8 [==============================] - 4s 488ms/step - loss: 12907.7790 - reconstruction_loss: 11939.4443 - kl_loss: 961.4215\n",
      "Epoch 38/4000\n",
      "8/8 [==============================] - 4s 459ms/step - loss: 12635.5717 - reconstruction_loss: 11723.1328 - kl_loss: 932.3978\n",
      "Epoch 39/4000\n",
      "8/8 [==============================] - 4s 460ms/step - loss: 12783.4060 - reconstruction_loss: 12020.4209 - kl_loss: 838.6907\n",
      "Epoch 40/4000\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 12529.8894 - reconstruction_loss: 11622.9229 - kl_loss: 863.3639\n",
      "Epoch 41/4000\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 12623.2221 - reconstruction_loss: 11735.9297 - kl_loss: 826.6069\n",
      "Epoch 42/4000\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 12503.3051 - reconstruction_loss: 11551.0059 - kl_loss: 874.8278\n",
      "Epoch 43/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 12309.8326 - reconstruction_loss: 11469.0645 - kl_loss: 791.5599\n",
      "Epoch 44/4000\n",
      "8/8 [==============================] - 4s 493ms/step - loss: 12325.8517 - reconstruction_loss: 11681.8164 - kl_loss: 687.7833\n",
      "Epoch 45/4000\n",
      "8/8 [==============================] - 4s 498ms/step - loss: 12540.0441 - reconstruction_loss: 11829.0996 - kl_loss: 825.8198\n",
      "Epoch 46/4000\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 12277.8051 - reconstruction_loss: 11311.3496 - kl_loss: 992.6665\n",
      "Epoch 47/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 12341.1999 - reconstruction_loss: 11504.2949 - kl_loss: 770.6171\n",
      "Epoch 48/4000\n",
      "8/8 [==============================] - 4s 510ms/step - loss: 12500.6366 - reconstruction_loss: 11568.8682 - kl_loss: 731.1600\n",
      "Epoch 49/4000\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 12120.8256 - reconstruction_loss: 11412.1045 - kl_loss: 730.0620\n",
      "Epoch 50/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 12016.8415 - reconstruction_loss: 11355.3115 - kl_loss: 701.9321\n",
      "Epoch 51/4000\n",
      "8/8 [==============================] - 4s 505ms/step - loss: 12258.1876 - reconstruction_loss: 11565.7363 - kl_loss: 699.1627\n",
      "Epoch 52/4000\n",
      "8/8 [==============================] - 4s 497ms/step - loss: 12111.2440 - reconstruction_loss: 11253.6748 - kl_loss: 788.0927\n",
      "Epoch 53/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 12025.9073 - reconstruction_loss: 11375.4902 - kl_loss: 668.7217\n",
      "Epoch 54/4000\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 11970.2622 - reconstruction_loss: 11359.2012 - kl_loss: 626.3689\n",
      "Epoch 55/4000\n",
      "8/8 [==============================] - 5s 529ms/step - loss: 11900.7561 - reconstruction_loss: 11261.6914 - kl_loss: 690.9596\n",
      "Epoch 56/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 11958.8307 - reconstruction_loss: 11229.2383 - kl_loss: 693.4360\n",
      "Epoch 57/4000\n",
      "8/8 [==============================] - 4s 478ms/step - loss: 11813.9414 - reconstruction_loss: 11205.9775 - kl_loss: 644.0386\n",
      "Epoch 58/4000\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 11847.9602 - reconstruction_loss: 11387.5801 - kl_loss: 589.2143\n",
      "Epoch 59/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 498ms/step - loss: 11865.2109 - reconstruction_loss: 11159.3398 - kl_loss: 665.0762\n",
      "Epoch 60/4000\n",
      "8/8 [==============================] - 5s 544ms/step - loss: 11685.5853 - reconstruction_loss: 11066.5635 - kl_loss: 643.6516\n",
      "Epoch 61/4000\n",
      "8/8 [==============================] - 5s 514ms/step - loss: 11791.3363 - reconstruction_loss: 11266.2412 - kl_loss: 520.5743\n",
      "Epoch 62/4000\n",
      "8/8 [==============================] - 5s 506ms/step - loss: 11782.6700 - reconstruction_loss: 11208.6357 - kl_loss: 532.9969\n",
      "Epoch 63/4000\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 11748.6238 - reconstruction_loss: 11135.1670 - kl_loss: 598.3072\n",
      "Epoch 64/4000\n",
      "8/8 [==============================] - 4s 562ms/step - loss: 11676.6424 - reconstruction_loss: 11067.9434 - kl_loss: 585.3838\n",
      "Epoch 65/4000\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 11650.9707 - reconstruction_loss: 11076.8291 - kl_loss: 518.5901\n",
      "Epoch 66/4000\n",
      "8/8 [==============================] - 5s 561ms/step - loss: 11682.8357 - reconstruction_loss: 11196.1758 - kl_loss: 484.4557\n",
      "Epoch 67/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 11541.1775 - reconstruction_loss: 11047.7754 - kl_loss: 548.5342\n",
      "Epoch 68/4000\n",
      "8/8 [==============================] - 4s 496ms/step - loss: 11602.9475 - reconstruction_loss: 11057.3164 - kl_loss: 543.5540\n",
      "Epoch 69/4000\n",
      "8/8 [==============================] - 4s 497ms/step - loss: 11562.7960 - reconstruction_loss: 11055.1475 - kl_loss: 491.4834\n",
      "Epoch 70/4000\n",
      "8/8 [==============================] - 5s 510ms/step - loss: 11812.5330 - reconstruction_loss: 11287.8867 - kl_loss: 456.8814\n",
      "Epoch 71/4000\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 11616.7537 - reconstruction_loss: 11064.9316 - kl_loss: 574.2426\n",
      "Epoch 72/4000\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 11566.5785 - reconstruction_loss: 10995.3965 - kl_loss: 523.1505\n",
      "Epoch 73/4000\n",
      "8/8 [==============================] - 4s 519ms/step - loss: 11448.6012 - reconstruction_loss: 11034.2324 - kl_loss: 409.5634\n",
      "Epoch 74/4000\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 11508.1348 - reconstruction_loss: 11085.0635 - kl_loss: 434.2992\n",
      "Epoch 75/4000\n",
      "8/8 [==============================] - 4s 488ms/step - loss: 11486.1683 - reconstruction_loss: 10957.8682 - kl_loss: 507.7236\n",
      "Epoch 76/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 11553.5381 - reconstruction_loss: 10997.2041 - kl_loss: 486.4631\n",
      "Epoch 77/4000\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 11389.0082 - reconstruction_loss: 11041.1104 - kl_loss: 405.6350\n",
      "Epoch 78/4000\n",
      "8/8 [==============================] - 5s 549ms/step - loss: 11531.6351 - reconstruction_loss: 11059.4980 - kl_loss: 428.4572\n",
      "Epoch 79/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 11570.1974 - reconstruction_loss: 10992.3916 - kl_loss: 506.2407\n",
      "Epoch 80/4000\n",
      "8/8 [==============================] - 4s 472ms/step - loss: 11448.1922 - reconstruction_loss: 10983.4707 - kl_loss: 466.5789\n",
      "Epoch 81/4000\n",
      "8/8 [==============================] - 4s 485ms/step - loss: 11397.6715 - reconstruction_loss: 11018.3105 - kl_loss: 392.6244\n",
      "Epoch 82/4000\n",
      "8/8 [==============================] - 4s 501ms/step - loss: 11365.0554 - reconstruction_loss: 10966.5723 - kl_loss: 389.3787\n",
      "Epoch 83/4000\n",
      "8/8 [==============================] - 4s 486ms/step - loss: 11461.7918 - reconstruction_loss: 11132.3535 - kl_loss: 391.6349\n",
      "Epoch 84/4000\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 11322.2259 - reconstruction_loss: 10903.0801 - kl_loss: 462.2806\n",
      "Epoch 85/4000\n",
      "8/8 [==============================] - 4s 510ms/step - loss: 11350.7435 - reconstruction_loss: 10882.9170 - kl_loss: 442.0450\n",
      "Epoch 86/4000\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 11397.0461 - reconstruction_loss: 11020.4785 - kl_loss: 356.1278\n",
      "Epoch 87/4000\n",
      "8/8 [==============================] - 4s 494ms/step - loss: 11274.5314 - reconstruction_loss: 10940.1230 - kl_loss: 356.2638\n",
      "Epoch 88/4000\n",
      "8/8 [==============================] - 4s 510ms/step - loss: 11304.0129 - reconstruction_loss: 10987.2070 - kl_loss: 377.9375\n",
      "Epoch 89/4000\n",
      "8/8 [==============================] - 5s 517ms/step - loss: 11401.9171 - reconstruction_loss: 10929.8926 - kl_loss: 426.6947\n",
      "Epoch 90/4000\n",
      "8/8 [==============================] - 4s 482ms/step - loss: 11268.7538 - reconstruction_loss: 10865.2734 - kl_loss: 391.4406\n",
      "Epoch 91/4000\n",
      "8/8 [==============================] - 4s 506ms/step - loss: 11279.1197 - reconstruction_loss: 10952.1641 - kl_loss: 331.4858\n",
      "Epoch 92/4000\n",
      "8/8 [==============================] - 4s 503ms/step - loss: 11244.1794 - reconstruction_loss: 10897.8428 - kl_loss: 339.2941\n",
      "Epoch 93/4000\n",
      "8/8 [==============================] - 4s 496ms/step - loss: 11391.2330 - reconstruction_loss: 10993.0518 - kl_loss: 347.1430\n",
      "Epoch 94/4000\n",
      "8/8 [==============================] - 4s 510ms/step - loss: 11334.9038 - reconstruction_loss: 10925.4658 - kl_loss: 397.7483\n",
      "Epoch 95/4000\n",
      "8/8 [==============================] - 4s 487ms/step - loss: 11345.2334 - reconstruction_loss: 10907.4736 - kl_loss: 403.0869\n",
      "Epoch 96/4000\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 11193.3161 - reconstruction_loss: 10873.4570 - kl_loss: 335.3958\n",
      "Epoch 97/4000\n",
      "8/8 [==============================] - 4s 519ms/step - loss: 11371.3178 - reconstruction_loss: 10988.1660 - kl_loss: 317.5589\n",
      "Epoch 98/4000\n",
      "8/8 [==============================] - 5s 526ms/step - loss: 11222.0326 - reconstruction_loss: 10906.1104 - kl_loss: 346.8278\n",
      "Epoch 99/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 11191.3306 - reconstruction_loss: 10830.0283 - kl_loss: 347.3010\n",
      "Epoch 100/4000\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 11181.2456 - reconstruction_loss: 10861.3848 - kl_loss: 303.5619\n",
      "Epoch 101/4000\n",
      "8/8 [==============================] - 4s 505ms/step - loss: 11220.0418 - reconstruction_loss: 10917.4766 - kl_loss: 283.1942\n",
      "Epoch 102/4000\n",
      "8/8 [==============================] - 5s 519ms/step - loss: 11313.1581 - reconstruction_loss: 10926.5098 - kl_loss: 339.0824\n",
      "Epoch 103/4000\n",
      "8/8 [==============================] - 4s 497ms/step - loss: 11147.1074 - reconstruction_loss: 10821.5098 - kl_loss: 361.9269\n",
      "Epoch 104/4000\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 11150.8831 - reconstruction_loss: 10896.8994 - kl_loss: 279.3630\n",
      "Epoch 105/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 11266.4856 - reconstruction_loss: 10952.0908 - kl_loss: 278.4377\n",
      "Epoch 106/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 11167.9634 - reconstruction_loss: 10829.8350 - kl_loss: 327.6903\n",
      "Epoch 107/4000\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 11161.5152 - reconstruction_loss: 10839.5137 - kl_loss: 318.0361\n",
      "Epoch 108/4000\n",
      "8/8 [==============================] - 5s 537ms/step - loss: 11198.3439 - reconstruction_loss: 10865.8545 - kl_loss: 295.3935\n",
      "Epoch 109/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 11193.7869 - reconstruction_loss: 10875.3691 - kl_loss: 293.3141\n",
      "Epoch 110/4000\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 11153.6780 - reconstruction_loss: 10832.0400 - kl_loss: 300.4829\n",
      "Epoch 111/4000\n",
      "8/8 [==============================] - 5s 523ms/step - loss: 11168.6201 - reconstruction_loss: 10849.6309 - kl_loss: 292.3887\n",
      "Epoch 112/4000\n",
      "8/8 [==============================] - 4s 494ms/step - loss: 11158.3641 - reconstruction_loss: 10842.8516 - kl_loss: 274.7604\n",
      "Epoch 113/4000\n",
      "8/8 [==============================] - 4s 478ms/step - loss: 11087.5291 - reconstruction_loss: 10840.1611 - kl_loss: 274.3675\n",
      "Epoch 114/4000\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 11050.9426 - reconstruction_loss: 10836.7910 - kl_loss: 283.0235\n",
      "Epoch 115/4000\n",
      "8/8 [==============================] - 5s 527ms/step - loss: 11112.9640 - reconstruction_loss: 10878.4111 - kl_loss: 273.0389\n",
      "Epoch 116/4000\n",
      "8/8 [==============================] - 5s 528ms/step - loss: 11118.5449 - reconstruction_loss: 10819.7178 - kl_loss: 292.3781\n",
      "Epoch 117/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 503ms/step - loss: 11050.9042 - reconstruction_loss: 10808.8516 - kl_loss: 283.5725\n",
      "Epoch 118/4000\n",
      "8/8 [==============================] - 5s 554ms/step - loss: 11073.5199 - reconstruction_loss: 10822.1055 - kl_loss: 266.0821\n",
      "Epoch 119/4000\n",
      "8/8 [==============================] - 5s 583ms/step - loss: 11126.5544 - reconstruction_loss: 10885.8311 - kl_loss: 244.8295\n",
      "Epoch 120/4000\n",
      "8/8 [==============================] - 5s 527ms/step - loss: 11093.8477 - reconstruction_loss: 10810.7012 - kl_loss: 253.3165\n",
      "Epoch 121/4000\n",
      "8/8 [==============================] - 4s 536ms/step - loss: 11065.2028 - reconstruction_loss: 10785.9043 - kl_loss: 257.2531\n",
      "Epoch 122/4000\n",
      "8/8 [==============================] - 4s 505ms/step - loss: 11057.0642 - reconstruction_loss: 10811.4492 - kl_loss: 243.2366\n",
      "Epoch 123/4000\n",
      "8/8 [==============================] - 5s 523ms/step - loss: 11092.8118 - reconstruction_loss: 10839.7959 - kl_loss: 237.2538\n",
      "Epoch 124/4000\n",
      "8/8 [==============================] - 5s 532ms/step - loss: 11075.7971 - reconstruction_loss: 10816.9824 - kl_loss: 250.6527\n",
      "Epoch 125/4000\n",
      "8/8 [==============================] - 4s 518ms/step - loss: 11029.9878 - reconstruction_loss: 10818.8721 - kl_loss: 263.2211\n",
      "Epoch 126/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 11057.6931 - reconstruction_loss: 10777.6250 - kl_loss: 263.3248\n",
      "Epoch 127/4000\n",
      "8/8 [==============================] - 5s 539ms/step - loss: 11084.8211 - reconstruction_loss: 10848.1123 - kl_loss: 246.1157\n",
      "Epoch 128/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 11051.9184 - reconstruction_loss: 10807.3535 - kl_loss: 233.5482\n",
      "Epoch 129/4000\n",
      "8/8 [==============================] - 5s 598ms/step - loss: 11031.8920 - reconstruction_loss: 10796.7705 - kl_loss: 227.8079\n",
      "Epoch 130/4000\n",
      "8/8 [==============================] - 5s 536ms/step - loss: 11115.2099 - reconstruction_loss: 10848.9316 - kl_loss: 231.6438\n",
      "Epoch 131/4000\n",
      "8/8 [==============================] - 4s 519ms/step - loss: 11109.5920 - reconstruction_loss: 10860.0156 - kl_loss: 244.3845\n",
      "Epoch 132/4000\n",
      "8/8 [==============================] - 4s 550ms/step - loss: 11079.2356 - reconstruction_loss: 10800.7568 - kl_loss: 254.3369\n",
      "Epoch 133/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 11050.8453 - reconstruction_loss: 10812.5615 - kl_loss: 224.7360\n",
      "Epoch 134/4000\n",
      "8/8 [==============================] - 5s 525ms/step - loss: 11009.2197 - reconstruction_loss: 10814.7354 - kl_loss: 209.0268\n",
      "Epoch 135/4000\n",
      "8/8 [==============================] - 5s 552ms/step - loss: 11081.5819 - reconstruction_loss: 10864.3330 - kl_loss: 221.5086\n",
      "Epoch 136/4000\n",
      "8/8 [==============================] - 5s 515ms/step - loss: 11033.6475 - reconstruction_loss: 10798.8555 - kl_loss: 246.8932\n",
      "Epoch 137/4000\n",
      "8/8 [==============================] - 4s 518ms/step - loss: 11027.6826 - reconstruction_loss: 10753.5049 - kl_loss: 239.8281\n",
      "Epoch 138/4000\n",
      "8/8 [==============================] - 5s 536ms/step - loss: 10968.6547 - reconstruction_loss: 10781.3418 - kl_loss: 194.5097\n",
      "Epoch 139/4000\n",
      "8/8 [==============================] - 4s 505ms/step - loss: 10994.3831 - reconstruction_loss: 10802.9805 - kl_loss: 187.8985\n",
      "Epoch 140/4000\n",
      "8/8 [==============================] - 5s 526ms/step - loss: 11026.9529 - reconstruction_loss: 10840.7109 - kl_loss: 209.6351\n",
      "Epoch 141/4000\n",
      "8/8 [==============================] - 5s 518ms/step - loss: 11012.3649 - reconstruction_loss: 10767.3242 - kl_loss: 223.7307\n",
      "Epoch 142/4000\n",
      "8/8 [==============================] - 5s 520ms/step - loss: 11005.1694 - reconstruction_loss: 10753.7666 - kl_loss: 215.7322\n",
      "Epoch 143/4000\n",
      "8/8 [==============================] - 5s 545ms/step - loss: 11027.9748 - reconstruction_loss: 10849.7832 - kl_loss: 200.7726\n",
      "Epoch 144/4000\n",
      "8/8 [==============================] - 5s 524ms/step - loss: 10982.0203 - reconstruction_loss: 10757.2900 - kl_loss: 209.4751\n",
      "Epoch 145/4000\n",
      "8/8 [==============================] - 5s 541ms/step - loss: 10982.9520 - reconstruction_loss: 10768.6318 - kl_loss: 199.6113\n",
      "Epoch 146/4000\n",
      "8/8 [==============================] - 5s 555ms/step - loss: 10950.1026 - reconstruction_loss: 10776.4160 - kl_loss: 184.5039\n",
      "Epoch 147/4000\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 10952.1284 - reconstruction_loss: 10797.1729 - kl_loss: 174.4791\n",
      "Epoch 148/4000\n",
      "8/8 [==============================] - 5s 529ms/step - loss: 10936.9053 - reconstruction_loss: 10767.4502 - kl_loss: 183.7159\n",
      "Epoch 149/4000\n",
      "8/8 [==============================] - 5s 535ms/step - loss: 10952.4840 - reconstruction_loss: 10758.3066 - kl_loss: 190.3487\n",
      "Epoch 150/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 11003.1039 - reconstruction_loss: 10792.5107 - kl_loss: 189.7178\n",
      "Epoch 151/4000\n",
      "8/8 [==============================] - 5s 523ms/step - loss: 10969.6937 - reconstruction_loss: 10749.0918 - kl_loss: 191.2066\n",
      "Epoch 152/4000\n",
      "8/8 [==============================] - 5s 524ms/step - loss: 10954.7305 - reconstruction_loss: 10785.1973 - kl_loss: 192.1673\n",
      "Epoch 153/4000\n",
      "8/8 [==============================] - 5s 540ms/step - loss: 10958.5534 - reconstruction_loss: 10773.2168 - kl_loss: 190.9373\n",
      "Epoch 154/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 10944.0390 - reconstruction_loss: 10741.2354 - kl_loss: 188.0532\n",
      "Epoch 155/4000\n",
      "8/8 [==============================] - 4s 505ms/step - loss: 10990.5500 - reconstruction_loss: 10805.3906 - kl_loss: 177.8847\n",
      "Epoch 156/4000\n",
      "8/8 [==============================] - 4s 541ms/step - loss: 10913.6743 - reconstruction_loss: 10760.4668 - kl_loss: 173.4533\n",
      "Epoch 157/4000\n",
      "8/8 [==============================] - 5s 561ms/step - loss: 10948.5710 - reconstruction_loss: 10774.4053 - kl_loss: 170.2149\n",
      "Epoch 158/4000\n",
      "8/8 [==============================] - 5s 549ms/step - loss: 10950.0909 - reconstruction_loss: 10770.4053 - kl_loss: 175.3643\n",
      "Epoch 159/4000\n",
      "8/8 [==============================] - 4s 514ms/step - loss: 10966.3147 - reconstruction_loss: 10793.3643 - kl_loss: 169.0874\n",
      "Epoch 160/4000\n",
      "8/8 [==============================] - 5s 520ms/step - loss: 10878.1619 - reconstruction_loss: 10740.1416 - kl_loss: 169.6530\n",
      "Epoch 161/4000\n",
      "8/8 [==============================] - 5s 570ms/step - loss: 10903.8376 - reconstruction_loss: 10770.1562 - kl_loss: 165.0812\n",
      "Epoch 162/4000\n",
      "8/8 [==============================] - 5s 523ms/step - loss: 10899.7536 - reconstruction_loss: 10752.7754 - kl_loss: 155.2534\n",
      "Epoch 163/4000\n",
      "8/8 [==============================] - 4s 514ms/step - loss: 10918.3707 - reconstruction_loss: 10798.2627 - kl_loss: 155.1577\n",
      "Epoch 164/4000\n",
      "8/8 [==============================] - 4s 499ms/step - loss: 10913.8026 - reconstruction_loss: 10746.7656 - kl_loss: 169.4876\n",
      "Epoch 165/4000\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 10973.9237 - reconstruction_loss: 10752.4746 - kl_loss: 174.4037\n",
      "Epoch 166/4000\n",
      "8/8 [==============================] - 5s 522ms/step - loss: 10892.4681 - reconstruction_loss: 10725.6514 - kl_loss: 163.8466\n",
      "Epoch 167/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 10901.8596 - reconstruction_loss: 10767.6357 - kl_loss: 155.2970\n",
      "Epoch 168/4000\n",
      "8/8 [==============================] - 4s 516ms/step - loss: 10858.2423 - reconstruction_loss: 10732.9453 - kl_loss: 145.4429\n",
      "Epoch 169/4000\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 10876.0484 - reconstruction_loss: 10764.9775 - kl_loss: 143.6884\n",
      "Epoch 170/4000\n",
      "8/8 [==============================] - 4s 514ms/step - loss: 10903.3511 - reconstruction_loss: 10752.1357 - kl_loss: 151.1491\n",
      "Epoch 171/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 10879.3677 - reconstruction_loss: 10703.3164 - kl_loss: 154.2271\n",
      "Epoch 172/4000\n",
      "8/8 [==============================] - 5s 538ms/step - loss: 10855.6063 - reconstruction_loss: 10726.8584 - kl_loss: 141.5283\n",
      "Epoch 173/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 10904.4781 - reconstruction_loss: 10764.3662 - kl_loss: 138.3499\n",
      "Epoch 174/4000\n",
      "8/8 [==============================] - 5s 518ms/step - loss: 10921.7347 - reconstruction_loss: 10742.2695 - kl_loss: 145.9655\n",
      "Epoch 175/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 518ms/step - loss: 10850.4533 - reconstruction_loss: 10758.0625 - kl_loss: 146.3952\n",
      "Epoch 176/4000\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 10923.2829 - reconstruction_loss: 10790.0742 - kl_loss: 140.3138\n",
      "Epoch 177/4000\n",
      "8/8 [==============================] - 4s 481ms/step - loss: 10897.4060 - reconstruction_loss: 10745.8877 - kl_loss: 138.2138\n",
      "Epoch 178/4000\n",
      "8/8 [==============================] - 5s 570ms/step - loss: 10877.5371 - reconstruction_loss: 10753.7441 - kl_loss: 139.4097\n",
      "Epoch 179/4000\n",
      "8/8 [==============================] - 5s 561ms/step - loss: 10862.9248 - reconstruction_loss: 10740.9238 - kl_loss: 141.1853\n",
      "Epoch 180/4000\n",
      "8/8 [==============================] - 5s 517ms/step - loss: 10900.8887 - reconstruction_loss: 10789.6289 - kl_loss: 139.9361\n",
      "Epoch 181/4000\n",
      "8/8 [==============================] - 4s 500ms/step - loss: 10874.5694 - reconstruction_loss: 10737.2588 - kl_loss: 143.7048\n",
      "Epoch 182/4000\n",
      "8/8 [==============================] - 5s 563ms/step - loss: 10843.6900 - reconstruction_loss: 10726.8672 - kl_loss: 137.9288\n",
      "Epoch 183/4000\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 10881.8387 - reconstruction_loss: 10745.9082 - kl_loss: 129.0567\n",
      "Epoch 184/4000\n",
      "8/8 [==============================] - 5s 520ms/step - loss: 10927.5114 - reconstruction_loss: 10771.2969 - kl_loss: 136.2642\n",
      "Epoch 185/4000\n",
      "8/8 [==============================] - 5s 589ms/step - loss: 10872.0409 - reconstruction_loss: 10716.5889 - kl_loss: 146.1774\n",
      "Epoch 186/4000\n",
      "8/8 [==============================] - 5s 546ms/step - loss: 10904.3319 - reconstruction_loss: 10755.5977 - kl_loss: 130.3583\n",
      "Epoch 187/4000\n",
      "8/8 [==============================] - 5s 532ms/step - loss: 10931.6559 - reconstruction_loss: 10760.1045 - kl_loss: 128.4048\n",
      "Epoch 188/4000\n",
      "8/8 [==============================] - 4s 494ms/step - loss: 10854.0016 - reconstruction_loss: 10725.8887 - kl_loss: 135.8971\n",
      "Epoch 189/4000\n",
      "8/8 [==============================] - 4s 518ms/step - loss: 10837.1518 - reconstruction_loss: 10730.5713 - kl_loss: 126.5920\n",
      "Epoch 190/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 10845.7125 - reconstruction_loss: 10734.9678 - kl_loss: 118.3011\n",
      "Epoch 191/4000\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 10849.1616 - reconstruction_loss: 10751.5449 - kl_loss: 123.4617\n",
      "Epoch 192/4000\n",
      "8/8 [==============================] - 5s 550ms/step - loss: 10839.2326 - reconstruction_loss: 10735.4736 - kl_loss: 126.7814\n",
      "Epoch 193/4000\n",
      "8/8 [==============================] - 5s 537ms/step - loss: 10843.6275 - reconstruction_loss: 10740.8906 - kl_loss: 129.4484\n",
      "Epoch 194/4000\n",
      "8/8 [==============================] - 5s 571ms/step - loss: 10872.8336 - reconstruction_loss: 10730.9189 - kl_loss: 127.9527\n",
      "Epoch 195/4000\n",
      "8/8 [==============================] - 5s 528ms/step - loss: 10828.0959 - reconstruction_loss: 10720.2754 - kl_loss: 123.5657\n",
      "Epoch 196/4000\n",
      "8/8 [==============================] - 5s 545ms/step - loss: 10867.2099 - reconstruction_loss: 10738.0557 - kl_loss: 126.2614\n",
      "Epoch 197/4000\n",
      "8/8 [==============================] - 5s 560ms/step - loss: 10868.0126 - reconstruction_loss: 10729.4062 - kl_loss: 121.2856\n",
      "Epoch 198/4000\n",
      "8/8 [==============================] - 5s 531ms/step - loss: 10936.5356 - reconstruction_loss: 10786.4336 - kl_loss: 122.4428\n",
      "Epoch 199/4000\n",
      "8/8 [==============================] - 5s 522ms/step - loss: 10850.7381 - reconstruction_loss: 10700.6553 - kl_loss: 136.6419\n",
      "Epoch 200/4000\n",
      "8/8 [==============================] - 5s 564ms/step - loss: 10884.7440 - reconstruction_loss: 10726.7275 - kl_loss: 129.3347\n",
      "Epoch 201/4000\n",
      "8/8 [==============================] - 5s 543ms/step - loss: 10861.5239 - reconstruction_loss: 10733.5488 - kl_loss: 121.9147\n",
      "Epoch 202/4000\n",
      "8/8 [==============================] - 5s 582ms/step - loss: 10915.1961 - reconstruction_loss: 10742.3271 - kl_loss: 121.2838\n",
      "Epoch 203/4000\n",
      "8/8 [==============================] - 5s 559ms/step - loss: 10821.4715 - reconstruction_loss: 10717.1387 - kl_loss: 112.3889\n",
      "Epoch 204/4000\n",
      "8/8 [==============================] - 5s 561ms/step - loss: 10832.1412 - reconstruction_loss: 10720.6934 - kl_loss: 113.0535\n",
      "Epoch 205/4000\n",
      "8/8 [==============================] - 5s 551ms/step - loss: 10816.3158 - reconstruction_loss: 10708.4932 - kl_loss: 122.8538\n",
      "Epoch 206/4000\n",
      "8/8 [==============================] - 5s 536ms/step - loss: 10814.2766 - reconstruction_loss: 10703.2266 - kl_loss: 111.6100\n",
      "Epoch 207/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 10834.1851 - reconstruction_loss: 10736.2197 - kl_loss: 104.2286\n",
      "Epoch 208/4000\n",
      "8/8 [==============================] - 5s 545ms/step - loss: 10809.7858 - reconstruction_loss: 10727.4482 - kl_loss: 108.5676\n",
      "Epoch 209/4000\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 10837.9839 - reconstruction_loss: 10714.8086 - kl_loss: 117.3417\n",
      "Epoch 210/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 10864.6744 - reconstruction_loss: 10724.3721 - kl_loss: 107.2352\n",
      "Epoch 211/4000\n",
      "8/8 [==============================] - 4s 517ms/step - loss: 10766.7473 - reconstruction_loss: 10718.7129 - kl_loss: 106.1418\n",
      "Epoch 212/4000\n",
      "8/8 [==============================] - 5s 536ms/step - loss: 10799.4342 - reconstruction_loss: 10712.6992 - kl_loss: 102.5149\n",
      "Epoch 213/4000\n",
      "8/8 [==============================] - 4s 551ms/step - loss: 10821.7000 - reconstruction_loss: 10729.3076 - kl_loss: 108.9285\n",
      "Epoch 214/4000\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 10840.2765 - reconstruction_loss: 10718.3643 - kl_loss: 116.5612\n",
      "Epoch 215/4000\n",
      "8/8 [==============================] - 5s 528ms/step - loss: 10839.5353 - reconstruction_loss: 10717.6133 - kl_loss: 105.8419\n",
      "Epoch 216/4000\n",
      "8/8 [==============================] - 5s 575ms/step - loss: 10838.8928 - reconstruction_loss: 10734.7383 - kl_loss: 102.1288\n",
      "Epoch 217/4000\n",
      "8/8 [==============================] - 5s 537ms/step - loss: 10783.9140 - reconstruction_loss: 10669.2500 - kl_loss: 108.7849\n",
      "Epoch 218/4000\n",
      "8/8 [==============================] - 5s 583ms/step - loss: 10808.6140 - reconstruction_loss: 10704.1982 - kl_loss: 106.1471\n",
      "Epoch 219/4000\n",
      "8/8 [==============================] - 5s 565ms/step - loss: 10854.0497 - reconstruction_loss: 10727.8750 - kl_loss: 109.7972\n",
      "Epoch 220/4000\n",
      "8/8 [==============================] - 5s 527ms/step - loss: 10824.6357 - reconstruction_loss: 10700.3984 - kl_loss: 115.8125\n",
      "Epoch 221/4000\n",
      "8/8 [==============================] - 4s 484ms/step - loss: 10791.7998 - reconstruction_loss: 10703.5938 - kl_loss: 102.5897\n",
      "Epoch 222/4000\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 10799.8971 - reconstruction_loss: 10706.4121 - kl_loss: 100.6105\n",
      "Epoch 223/4000\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 10820.5975 - reconstruction_loss: 10712.5312 - kl_loss: 101.2179\n",
      "Epoch 224/4000\n",
      "8/8 [==============================] - 5s 532ms/step - loss: 10822.8378 - reconstruction_loss: 10701.0010 - kl_loss: 106.9475\n",
      "Epoch 225/4000\n",
      "8/8 [==============================] - 4s 508ms/step - loss: 10765.9778 - reconstruction_loss: 10680.7109 - kl_loss: 105.8468\n",
      "Epoch 226/4000\n",
      "8/8 [==============================] - 4s 518ms/step - loss: 10781.8079 - reconstruction_loss: 10692.6729 - kl_loss: 96.3559\n",
      "Epoch 227/4000\n",
      "8/8 [==============================] - 5s 529ms/step - loss: 10803.5362 - reconstruction_loss: 10705.1973 - kl_loss: 93.0721\n",
      "Epoch 228/4000\n",
      "8/8 [==============================] - 5s 540ms/step - loss: 10780.9832 - reconstruction_loss: 10707.2139 - kl_loss: 103.7614\n",
      "Epoch 229/4000\n",
      "8/8 [==============================] - 5s 527ms/step - loss: 10826.2738 - reconstruction_loss: 10708.4805 - kl_loss: 96.0835\n",
      "Epoch 230/4000\n",
      "8/8 [==============================] - 5s 530ms/step - loss: 10829.5252 - reconstruction_loss: 10715.1455 - kl_loss: 100.4742\n",
      "Epoch 231/4000\n",
      "8/8 [==============================] - 5s 541ms/step - loss: 10762.5445 - reconstruction_loss: 10682.3271 - kl_loss: 104.9843\n",
      "Epoch 232/4000\n",
      "8/8 [==============================] - 5s 544ms/step - loss: 10824.5229 - reconstruction_loss: 10698.0605 - kl_loss: 95.0488\n",
      "Epoch 233/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 530ms/step - loss: 10792.2812 - reconstruction_loss: 10698.8945 - kl_loss: 91.6563\n",
      "Epoch 234/4000\n",
      "8/8 [==============================] - 5s 523ms/step - loss: 10802.6420 - reconstruction_loss: 10685.3066 - kl_loss: 101.3161\n",
      "Epoch 235/4000\n",
      "8/8 [==============================] - 4s 504ms/step - loss: 10797.2614 - reconstruction_loss: 10694.6826 - kl_loss: 94.7016\n",
      "Epoch 236/4000\n",
      "8/8 [==============================] - 4s 512ms/step - loss: 10835.0765 - reconstruction_loss: 10731.1055 - kl_loss: 93.9548\n",
      "Epoch 237/4000\n",
      "8/8 [==============================] - 5s 566ms/step - loss: 10856.6451 - reconstruction_loss: 10706.7002 - kl_loss: 111.9420\n",
      "Epoch 238/4000\n",
      "8/8 [==============================] - 4s 518ms/step - loss: 10797.9714 - reconstruction_loss: 10709.3262 - kl_loss: 95.9316\n",
      "Epoch 239/4000\n",
      "8/8 [==============================] - 4s 536ms/step - loss: 10831.0242 - reconstruction_loss: 10713.7256 - kl_loss: 86.0274\n",
      "Epoch 240/4000\n",
      "8/8 [==============================] - 4s 515ms/step - loss: 10769.5905 - reconstruction_loss: 10705.0645 - kl_loss: 92.5540\n",
      "Epoch 241/4000\n",
      "8/8 [==============================] - 4s 509ms/step - loss: 10782.9151 - reconstruction_loss: 10701.1025 - kl_loss: 105.5227\n",
      "Epoch 242/4000\n",
      "8/8 [==============================] - 5s 526ms/step - loss: 10789.6118 - reconstruction_loss: 10692.7109 - kl_loss: 97.1519\n",
      "Epoch 243/4000\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 10804.0199 - reconstruction_loss: 10673.4346 - kl_loss: 95.0765\n",
      "Epoch 244/4000\n",
      "8/8 [==============================] - 5s 527ms/step - loss: 10792.5554 - reconstruction_loss: 10701.6582 - kl_loss: 87.9631\n",
      "Epoch 245/4000\n",
      "8/8 [==============================] - 5s 540ms/step - loss: 10812.1879 - reconstruction_loss: 10700.0566 - kl_loss: 89.0323\n",
      "Epoch 246/4000\n",
      "8/8 [==============================] - 4s 507ms/step - loss: 10744.6660 - reconstruction_loss: 10684.1152 - kl_loss: 100.1187\n",
      "Epoch 247/4000\n",
      "8/8 [==============================] - 4s 519ms/step - loss: 10769.7206 - reconstruction_loss: 10664.1475 - kl_loss: 92.0970\n",
      "Epoch 248/4000\n",
      "8/8 [==============================] - 5s 526ms/step - loss: 10803.3741 - reconstruction_loss: 10693.7109 - kl_loss: 82.2226\n",
      "Epoch 249/4000\n",
      "8/8 [==============================] - 5s 524ms/step - loss: 10745.0560 - reconstruction_loss: 10680.5684 - kl_loss: 92.0855\n",
      "Epoch 250/4000\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 10796.5164 - reconstruction_loss: 10687.8633 - kl_loss: 92.6466\n",
      "Epoch 251/4000\n",
      "8/8 [==============================] - 4s 552ms/step - loss: 10820.3383 - reconstruction_loss: 10680.2676 - kl_loss: 91.7493\n",
      "Epoch 252/4000\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 10778.7670 - reconstruction_loss: 10673.6025 - kl_loss: 87.8645\n",
      "Epoch 253/4000\n",
      "8/8 [==============================] - 5s 530ms/step - loss: 10751.2383 - reconstruction_loss: 10689.4277 - kl_loss: 91.1028\n",
      "Epoch 254/4000\n",
      "8/8 [==============================] - 5s 526ms/step - loss: 10753.0230 - reconstruction_loss: 10661.8760 - kl_loss: 92.7522\n",
      "Epoch 255/4000\n",
      "5/8 [=================>............] - ETA: 1s - loss: 10777.7654 - reconstruction_loss: 10694.6934 - kl_loss: 84.1760"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.00001)\n",
    "#nll = lambda x , rv_x: -rv_x.log_prob(x)\n",
    "vae.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "vae.fit(train_generator_IMAGES, epochs=4000, \n",
    "                batch_size=16,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.encoder.save('Models/GVAE_DRY_MF1_128_1_encoder.h5')\n",
    "vae.decoder.save('Models/GVAE_DRY_MF1_128_1_decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23cb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
